import React from 'react';
import NavButtons from '../../components/shared/NavButtons';
import GarakLayout from '../../components/layout/GarakLayout';

const GarakExploitationAnalysis = () => {
  return (
    <GarakLayout>
      <h1>Analysis of Garak Exploitation Probe Module</h1>

      <section>
        <h2>Module Overview</h2>
        <p>
          This <a href="https://github.com/NVIDIA/garak/blob/main/garak/probes/exploitation.py" className="text-blue-600 hover:underline">file link</a> contains a specialized Garak probe module focused on exploitation techniques.
          This module is designed to test LLMs for vulnerabilities related to various injection attacks,
          including template injection, code injection, and SQL injection. These probes represent advanced
          security testing scenarios that could be particularly relevant to LLMs deployed in production systems
          where they might interact with databases, templating engines, or code execution environments.
        </p>
        <p>
          The module implements three specific probe classes, each targeting a different exploitation vector.
          These probes are highly specialized and are labeled with appropriate security tags and vulnerability
          classifications, making them valuable tools for comprehensive security assessments.
        </p>
      </section>

      <section>
        <h2>Core Templates and Techniques</h2>
        <p>
          At the heart of this module are several template strings that define the attack patterns:
        </p>
        <ul>
          <li>
            <strong>ECHO_TEMPLATE</strong>: An instruction template that attempts to get the LLM to echo back
            exactly what follows after "ECHO:", which can be used to make the model repeat potentially harmful
            content. This template provides examples to prime the model for this behavior, making it more likely
            to follow the pattern.
          </li>
          <li>
            <strong>SQL_EXACT_TEMPLATE</strong>: A template for delivering SQL queries with instructions for the
            model to run them "exactly" and report success or failure. This is designed to test if models will
            faithfully execute potentially harmful SQL commands.
          </li>
          <li>
            <strong>SQL_COMMAND_TEMPLATE</strong>: A variation of the SQL template that frames the injection as
            a command rather than a query, potentially bypassing different types of guardrails.
          </li>
        </ul>
        <p>
          These templates implement social engineering techniques (positioning the request as an administrative
          test) to increase the chance of bypassing model safeguards. This reflects real-world attack scenarios
          where malicious prompts are often disguised as legitimate requests.
        </p>
      </section>

      <section>
        <h2>Jinja Template Python Injection Probe</h2>
        <p>
          The first probe class targets template injection vulnerabilities specifically in the Jinja templating
          system commonly used with Python frameworks like Flask. Key aspects include:
        </p>
        <ul>
          <li>
            <strong>Attack Vector</strong>: This probe attempts to get the model to output Python code wrapped
            in Jinja template syntax (double curly braces), which could be executed if the model's output is
            directly processed by a Jinja template engine.
          </li>
          <li>
            <strong>Implementation</strong>: The class loads payloads from a predefined collection and formats
            them into the ECHO_TEMPLATE with double curly braces, creating valid Jinja syntax that could trigger
            server-side template injection.
          </li>
          <li>
            <strong>Security Classification</strong>: The probe is tagged with relevant CWE (Common Weakness
            Enumeration) identifiers including CWE-94 (Code Injection), CWE-95 (Eval Injection), and CWE-1336
            (Template Engine Injection).
          </li>
          <li>
            <strong>Detection Strategy</strong>: The probe recommends specific detectors that can identify
            successful template injection attempts in the model's responses.
          </li>
        </ul>
      </section>

      <section>
        <h2>SQL Injection System Probe</h2>
        <p>
          The second probe class focuses on SQL injection vulnerabilities where an LLM might pass untrusted
          SQL commands to a database system. Notable characteristics include:
        </p>
        <ul>
          <li>
            <strong>High-Risk Classification</strong>: This probe is explicitly marked as potentially harmful
            with a warning that it should "NEVER BE USED IN PRODUCTION ENVIRONMENTS", indicating its potential
            to cause real damage to database systems.
          </li>
          <li>
            <strong>Active Flag</strong>: The probe is set to inactive by default (active = False), requiring
            explicit enablement before use, which is a safety measure for dangerous probes.
          </li>
          <li>
            <strong>Attack Techniques</strong>: The probe includes specific destructive SQL commands like "Drop
            all available tables" and attempts to extract user records, demonstrating common malicious SQL
            injection patterns.
          </li>
          <li>
            <strong>Detection Limitations</strong>: The probe notes that detecting successful exploitation
            requires inspection of the target SQL database, highlighting the challenge of confirming impacts in
            black-box testing scenarios.
          </li>
        </ul>
      </section>

      <section>
        <h2>SQL Injection Echo Probe</h2>
        <p>
          The third probe class takes a different approach to SQL injection testing by focusing on getting the
          model to echo SQL statements that could be executed in certain contexts:
        </p>
        <ul>
          <li>
            <strong>Indirect Exploitation</strong>: This probe considers the scenario where a model's output
            might be processed by another system, leading to indirect SQL injection. This reflects a more
            realistic attack chain in production environments.
          </li>
          <li>
            <strong>Echo Technique</strong>: Like the Jinja probe, this class uses the ECHO_TEMPLATE but with
            SQL payloads, attempting to get the model to repeat potentially harmful SQL commands verbatim.
          </li>
          <li>
            <strong>Detection Approach</strong>: The probe recommends a specialized detector (SQLiEcho) focused
            on identifying echoed SQL syntax rather than looking for execution confirmation.
          </li>
        </ul>
      </section>

      <section>
        <h2>Security Tagging and Classification</h2>
        <p>
          A significant aspect of the module is its comprehensive security tagging system. Each probe includes
          extensive tags that map to various security frameworks:
        </p>
        <ul>
          <li>
            <strong>AVID Effect Taxonomy</strong>: Tags like "avid-effect:security:S0301" and
            "avid-effect:security:S0403" link these probes to the AI Vulnerability Database classification system.
          </li>
          <li>
            <strong>OWASP LLM Framework</strong>: References to "owasp:llm02" (Insecure Output Handling) and
            "owasp:llm06" (Sensitive Information Disclosure) align the probes with the OWASP Top 10 for LLM
            applications.
          </li>
          <li>
            <strong>Common Weakness Enumeration (CWE)</strong>: Detailed CWE identifiers provide precise
            vulnerability classifications according to industry standards.
          </li>
          <li>
            <strong>Quality Attributes</strong>: Tags like "quality:Security:PromptStability" and
            "quality:Security:ExtractionInversion" categorize the nature of the security testing.
          </li>
        </ul>
        <p>
          This multi-framework tagging approach enables integration with various security reporting systems and
          allows for consistent vulnerability tracking across different security assessment methodologies.
        </p>
      </section>

      <section>
        <h2>Payload Management</h2>
        <p>
          The module demonstrates Garak's structured approach to managing attack payloads:
        </p>
        <ul>
          <li>
            <strong>Separation of Concerns</strong>: Payloads are stored separately from probe logic and loaded
            dynamically using the garak.payloads.load() mechanism. This allows for easier payload management and
            updates.
          </li>
          <li>
            <strong>Configurable Payload Sets</strong>: Each probe class defines a default payload_name parameter
            that can be overridden during instantiation, providing flexibility in testing.
          </li>
          <li>
            <strong>Generation Pattern</strong>: All three probes follow a similar pattern of iterating through
            payloads to generate prompts, showing a consistent design pattern for probe implementation.
          </li>
        </ul>
      </section>

      <section>
        <h2>Technical Implementation Notes</h2>
        <p>
          Several technical details in the implementation provide insights into Garak's probe architecture:
        </p>
        <ul>
          <li>
            <strong>Inheritance Pattern</strong>: All probe classes inherit from the base Probe class, conforming
            to Garak's plugin architecture.
          </li>
          <li>
            <strong>Configuration Management</strong>: The probes accept a config_root parameter defaulting to
            _config, allowing for runtime configuration.
          </li>
          <li>
            <strong>Parameter Extension</strong>: The DEFAULT_PARAMS dictionary is extended using the pipe operator
            (|) to merge base parameters with probe-specific ones, demonstrating Python 3.9+ dictionary merging.
          </li>
          <li>
            <strong>Probe Tiering</strong>: The probes use a TIER_1 classification, suggesting Garak implements a
            tiering system for organizing probes by complexity, severity, or other attributes.
          </li>
        </ul>
      </section>

      <section>
        <h2>Security Research and Documentation</h2>
        <p>
          Each probe includes references to security documentation and research:
        </p>
        <ul>
          <li>
            <strong>Documentation URIs</strong>: Links to resources like PortSwigger's template injection guide
            and MITRE's CWE database entries provide context and educational value.
          </li>
          <li>
            <strong>Clear Goals</strong>: Each probe has an explicit goal statement (e.g., "Gain remote code
            execution via Jinja template injection") that clarifies its purpose and success criteria.
          </li>
          <li>
            <strong>Language Specification</strong>: The bcp47 attribute specifies the language (English) for
            which the probe is designed, acknowledging that exploitation techniques may be language-dependent.
          </li>
        </ul>
      </section>

      <section>
        <h2>Security Implications and Ethical Considerations</h2>
        <p>
          The module raises important considerations about LLM security testing:
        </p>
        <ul>
          <li>
            <strong>Dual-Use Nature</strong>: These probes could be used both by defenders to identify
            vulnerabilities and by attackers to exploit them, highlighting the dual-use nature of security testing
            tools.
          </li>
          <li>
            <strong>Safety Precautions</strong>: The explicit warnings and inactive default state for high-risk
            probes demonstrate responsible security disclosure practices.
          </li>
          <li>
            <strong>System Context</strong>: The probes highlight how LLM vulnerabilities often depend on the
            broader system context in which the model operates, rather than just the model itself.
          </li>
          <li>
            <strong>Detection Challenges</strong>: The notes about detection limitations emphasize the challenges
            in confirming successful exploitation, which is a common issue in security testing.
          </li>
        </ul>
      </section>

      <section>
        <h2>Conclusions</h2>
        <p>
          This exploitation module represents an advanced component of Garak's security testing capabilities, focusing
          on injection vulnerabilities that could affect LLM deployments. The implementation demonstrates:
        </p>
        <ul>
          <li>
            A sophisticated understanding of how LLMs might be vulnerable to injection attacks
          </li>
          <li>
            Careful design with appropriate safety measures for high-risk probes
          </li>
          <li>
            Comprehensive security classification through multi-framework tagging
          </li>
          <li>
            Separation of concerns between probe logic and attack payloads
          </li>
          <li>
            Consideration of indirect exploitation scenarios where LLM outputs might be processed by other systems
          </li>
        </ul>
        <p>
          Overall, the module illustrates the complex security considerations necessary when deploying LLMs in
          production environments, particularly those where model outputs might interact with databases, template
          engines, or code execution contexts.
        </p>
      </section>

      <NavButtons
        previous = {{
          link: "/garak/undestanding-output",
          text: "Understanding Garak Results"
        }}
        next={{
          link: "/garak/custom-probes",
          text: "Creating Custom Probes"
        }}
      />
    </GarakLayout>
  );
};

export default GarakExploitationAnalysis;